{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering and Improvement\n",
    "#### Task 5: Feature Engineering\n",
    "\n",
    "Notebook: notebooks/Feature_Engineering.ipynb\n",
    "Steps:\n",
    "- Create new features that might improve model performance.\n",
    "- Test different feature combinations.\n",
    "- Evaluate the impact of new features on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       crim        zn     indus      chas       nox        rm       age  \\\n",
      "0 -0.236041  0.871318 -0.501726 -0.260378 -1.073076  0.439527 -0.440185   \n",
      "1 -0.214499  0.237584 -0.144178 -0.260378  0.338419 -0.580839  0.940816   \n",
      "2  0.089608 -0.596277  2.349763 -0.260378  1.856155 -0.930475  1.402401   \n",
      "3  2.283708  0.737901 -0.837928 -0.260378  2.205235  2.565886  1.263550   \n",
      "4 -0.332314 -0.596277 -0.823697 -0.260378  0.125936 -1.608341  1.038387   \n",
      "\n",
      "        dis       rad       tax   ptratio         b     lstat  \n",
      "0  1.951028  1.484407  0.243996  0.358605 -2.474124 -0.268091  \n",
      "1  1.253798  0.281608 -0.003673 -1.785172 -0.692920  1.404934  \n",
      "2 -1.527156 -0.319792  1.638760  1.512946  0.098231  2.336354  \n",
      "3 -1.295462  0.281608 -0.616326 -2.994482  0.303070  0.113747  \n",
      "4 -1.003274  0.281608 -0.199200 -1.015611  0.819619  0.873009  \n",
      "       crim        zn     indus      chas       nox        rm       age  \\\n",
      "0  0.569941 -0.596277  0.339668  3.840573 -0.192789 -2.007925  1.042140   \n",
      "1 -0.561767 -0.596277  0.738129 -0.260378 -0.982012  0.058971 -2.057608   \n",
      "2  0.544853 -0.596277  0.216927 -0.260378  0.641966  0.758243  0.993354   \n",
      "3  0.294240 -0.596277 -0.231342 -0.260378 -0.132079  0.151732 -1.198235   \n",
      "4  1.480890 -0.596277 -0.441245 -0.260378  0.080404  0.879546  0.749427   \n",
      "\n",
      "        dis       rad       tax   ptratio         b     lstat  \n",
      "0 -0.399524 -0.319792 -0.446869  0.083762  0.573219  2.923457  \n",
      "1 -0.068250  0.281608  1.130388  0.138730  0.525720 -0.872851  \n",
      "2 -0.434939 -0.319792 -0.094919 -0.026176  0.640014 -0.321062  \n",
      "3  0.589442  0.281608 -0.316517  0.633448  0.819619 -1.011902  \n",
      "4 -0.621447  2.085806 -0.055813 -0.575862  0.819619 -0.691865  \n",
      "       medv\n",
      "0  0.404414\n",
      "1 -0.825827\n",
      "2 -2.056069\n",
      "3  1.832372\n",
      "4  0.096854\n",
      "       medv\n",
      "0 -0.737953\n",
      "1  0.316540\n",
      "2  0.250634\n",
      "3  0.074885\n",
      "4  1.634655\n"
     ]
    }
   ],
   "source": [
    "# # Load dataset\n",
    "file_path = (r\"D:\\Assignment\\Machine Lerning\\machine-learning-introduction-Sabuna-Gamal\\Data\\BostonHousing.csv\") \n",
    "df = pd.read_csv(file_path)  # Read CSV into a DataFrame\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "X_test = pd.read_csv(r\"D:\\Assignment\\Machine Lerning\\machine-learning-introduction-Sabuna-Gamal\\Data\\X_test.csv\")\n",
    "X_train = pd.read_csv(r\"D:\\Assignment\\Machine Lerning\\machine-learning-introduction-Sabuna-Gamal\\Data\\X_train.csv\")\n",
    "y_test = pd.read_csv(r\"D:\\Assignment\\Machine Lerning\\machine-learning-introduction-Sabuna-Gamal\\Data\\y_test.csv\")\n",
    "y_train = pd.read_csv(r\"D:\\Assignment\\Machine Lerning\\machine-learning-introduction-Sabuna-Gamal\\Data\\y_train.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(X_test.head())\n",
    "print(X_train.head())\n",
    "print(y_test.head())\n",
    "print(y_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before handling:\n",
      " crim       0\n",
      "zn         0\n",
      "indus      0\n",
      "chas       0\n",
      "nox        0\n",
      "rm         5\n",
      "age        0\n",
      "dis        0\n",
      "rad        0\n",
      "tax        0\n",
      "ptratio    0\n",
      "b          0\n",
      "lstat      0\n",
      "medv       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values before handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Drop rows with NaN values (only if few values are missing)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transformation (Handling Skewness)\n",
    "# Apply log transformation to features with high skewness\n",
    "df['lstat_log'] = np.log1p(df['lstat'])  # Log transformation\n",
    "df['rm_squared'] = df['rm'] ** 2  # Square transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial Features (Adding Complexity)\n",
    "# Select relevant features\n",
    "poly_features = ['lstat', 'rm', 'ptratio']\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_transformed = poly.fit_transform(df[poly_features])\n",
    "\n",
    "# Convert to DataFrame and add to original dataset\n",
    "poly_df = pd.DataFrame(poly_transformed, columns=poly.get_feature_names_out(poly_features))\n",
    "df = pd.concat([df, poly_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lstat', 'rm', 'ptratio'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns[df.columns.duplicated()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate columns if any\n",
    "df = df.loc[:, ~df.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###Train and Evaluate the Model with New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n",
      "       'ptratio', 'b', 'lstat', 'medv', 'lstat_log', 'rm_squared', 'lstat^2',\n",
      "       'lstat rm', 'lstat ptratio', 'rm^2', 'rm ptratio', 'ptratio^2',\n",
      "       'rm_lstat_interaction', 'age_dis_interaction'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Ensure correct data types\n",
    "df['rm'] = df['rm'].astype(float)\n",
    "df['lstat'] = df['lstat'].astype(float)\n",
    "\n",
    "# Create unique interaction feature names\n",
    "df['rm_lstat_interaction'] = df['rm'].values * df['lstat'].values\n",
    "df['age_dis_interaction'] = df['age'].values * df['dis'].values\n",
    "\n",
    "# Check if the new columns are correctly added\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming df is your DataFrame with features and target variable\n",
    "\n",
    "# Impute missing values in features (X) using mean for numerical features\n",
    "X_train = df.drop('medv', axis=1)  # Drop target column\n",
    "y_train = df['medv']  # Target column\n",
    "\n",
    "# Apply SimpleImputer to impute missing values in the features\n",
    "imputer = SimpleImputer(strategy='mean')  # You can change the strategy (e.g., median, mode)\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Impute target variable if it has missing values\n",
    "y_train_imputed = y_train.fillna(y_train.mean())  # Impute target using mean or median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Interaction Features (Feature Interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train a baseline model with original features\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_baseline \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mptratio\u001b[39m\u001b[38;5;124m'\u001b[39m]]  \u001b[38;5;66;03m# Using only basic features\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X_train_base, X_test_base, y_train_base, y_test_base \u001b[38;5;241m=\u001b[39m train_test_split(X_baseline, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train the baseline model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m baseline_model \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# Train a baseline model with original features\n",
    "X_baseline = df[['lstat', 'rm', 'ptratio']]  # Using only basic features\n",
    "X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_baseline, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train_base, y_train_base)\n",
    "y_pred_base = baseline_model.predict(X_test_base)\n",
    "\n",
    "# Evaluate baseline performance\n",
    "mse_base = mean_squared_error(y_test_base, y_pred_base)\n",
    "r2_base = r2_score(y_test_base, y_pred_base)\n",
    "\n",
    "print(f\"\\nBaseline Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_base:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_base:.2f}\")\n",
    "\n",
    "# Compare improvement\n",
    "improvement = ((r2 - r2_base) / r2_base) * 100\n",
    "print(f\"\\nImprovement in R² Score: {improvement:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
